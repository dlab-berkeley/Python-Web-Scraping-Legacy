{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Data: Some Preliminary Considerations\n",
    "\n",
    "Whenever you're trying to get information from the web, it's very important to first know whether you're accessing it through appropriate means.\n",
    "\n",
    "The UC Berkeley library has some excellent resources on this topic. Here is a flowchart that can help guide your course of action.\n",
    "\n",
    "![](figures/scraping_flowchart.png)\n",
    "\n",
    "You can see the library's licensed sources [here](http://guides.lib.berkeley.edu/text-mining)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping with Beautiful Soup\n",
    "*****\n",
    "\n",
    "\n",
    "## Intro\n",
    "\n",
    "In this tutorial, we'll be scraping information on the state senators of Illinois, available [here](http://www.ilga.gov/senate), as well as the list of bills each senator has sponsored (e.g., [here](http://www.ilga.gov/senate/SenatorBills.asp?MemberID=1911&GA=98&Primary=True)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tools\n",
    "\n",
    "1. [Requests](http://docs.python-requests.org/en/latest/user/quickstart/)\n",
    "2. [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't already installed Beautiful Soup, go to the command line and enter this command:\n",
    "\n",
    "`pip install beautifulsoup4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Using Beautiful Soup\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Make a GET Request and Read in HTML\n",
    "\n",
    "We can use the `requests` library to:\n",
    "1. make a GET request to the page\n",
    "2. read in the html of the page\n",
    "\n",
    "This should be somewhat familiar from when we used it with APIs. Now we're making a request directly to the website, and we're going to have to parse the html, instead of something more straightforward like json or xml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a GET request\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp')\n",
    "# read the content of the server’s response\n",
    "src = req.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Soup it\n",
    "\n",
    "Now we use the `BeautifulSoup` function to parse the reponse into an HTML tree. This returns an object (called a **soup object**) which contains all of the HTML in the original document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the response into an HTML tree\n",
    "soup = BeautifulSoup(src, 'lxml')\n",
    "# take a look\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Find Elements\n",
    "\n",
    "BeautifulSoup has a number of functions to find things on a page. Like other webscraping tools, Beautiful Soup lets you find elements by their:\n",
    "\n",
    "1. HTML tags\n",
    "2. HTML Attributes\n",
    "3. CSS Selectors\n",
    "\n",
    "\n",
    "Let's search first for **HTML tags**. \n",
    "\n",
    "The function `find_all` searches the `soup` tree to find all the elements with an a particular HTML tag, and returns all of those elements.\n",
    "\n",
    "What does the example below do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all elements with a certain tag\n",
    "\n",
    "soup.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB**: Because `find_all()` is the most popular method in the Beautiful Soup search API, you can use a shortcut for it. If you treat the BeautifulSoup object as though it were a function, then it’s the same as calling `find_all()` on that object. \n",
    "\n",
    "These two lines of code are equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup.find_all(\"a\")\n",
    "soup(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot! Many elements on a page will have the same html tag. For instance, if you search for everything with the `a` tag, you're likely to get a lot of stuff, much of which you don't want. Remember, the `a` tag defines a hyperlink, so they'rell often be a lot of those on a page.\n",
    "\n",
    "What if we wanted to search for HTML tags ONLY with certain attributes, like particular CSS classes? \n",
    "\n",
    "We can do this by adding an additional argument to the `find_all`\n",
    "\n",
    "In the example below, we are finding all the `a` tags, and then filtering those with `class = \"sidemenu\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get only the 'a' tags in 'sidemenu' class\n",
    "soup(\"a\", class_=\"sidemenu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oftentimes a more efficient way to search and find things on a website is by **CSS selector.** For this we have to use a different method, `select()`. Just pass a string into the `.select()` to get all elements with that string as a valid CSS selector.\n",
    "\n",
    "In the example above, we can use \"a.sidemenu\" as a CSS selector, which returns all `a` tags with class `sidemenu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get elements with \"a.sidemenu\" CSS Selector.\n",
    "soup.select(\"a.sidemenu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1\n",
    "\n",
    "Find all the `<a>` elements in class `mainmenu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Get Attributes and Text of Elements\n",
    "\n",
    "Once we identify elements, we want the access information in that element. Oftentimes this means two things:\n",
    "\n",
    "1. Text\n",
    "2. Attributes\n",
    "\n",
    "Getting the text inside an element is easy. All we have to do is use the `text` member of a `tag` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is a list\n",
    "soup.select(\"a.sidemenu\")\n",
    "\n",
    "# we first want to get an individual tag object\n",
    "first_link = soup.select(\"a.sidemenu\")[0]\n",
    "\n",
    "# check out its class\n",
    "type(first_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a tag! Which means it has a `text` member:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(first_link.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we want the value of certain attributes. This is particularly relevant for `a` tags, or links, where the `href` attribute tells us where the link goes.\n",
    "\n",
    "You can access a tag’s attributes by treating the tag like a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(first_link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2\n",
    "\n",
    "Find all the `href` attributes (url) from the mainmenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "****\n",
    "\n",
    "Believe it or not, those are really the fundamental tools you need to scrape a website. Once you spend more time familiarizing yourself with HTML and CSS, then it's simply a matter of understanding the structure of a particular website and intelligently applying the tools of BeautifulSoup and Python.\n",
    "\n",
    "Let's apply these skills to scrape http://www.ilga.gov/senate/default.asp?GA=98\n",
    "\n",
    "**NB: we're just going to scrape the 98th general assembly.**\n",
    "\n",
    "Our goal is to scrape information on each senator, including their:\n",
    "    - name\n",
    "    - district\n",
    "    - party\n",
    "\n",
    "## 2.1 First, make the get request and soup it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required modules from previous session\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a GET request\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\n",
    "# read the content of the server’s response\n",
    "src = req.text\n",
    "# soup it\n",
    "soup = BeautifulSoup(src, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Find the right elements and text.\n",
    "\n",
    "Now let's try to get a list of rows in that table. Remember that rows are identified by the `tr` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all tr elements\n",
    "rows = soup.find_all(\"tr\")\n",
    "len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But remember, `find_all` gets *all* the elements with the `tr` tag. We only want some of them. If we use the 'Inspect' function in Google Chrome and look carefully, then we can use some CSS selectors to get just the rows we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns every ‘tr tr tr’ css selector in the page\n",
    "rows = soup.select('tr tr tr')\n",
    "\n",
    "for r in rows[:5]:\n",
    "    print(r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we want everything after the first two rows. Let's work with a single row to start, and then from that we'll build our loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rows[2].prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break this row down into its component cells/columns using the `select` method with CSS selectors. Looking closely at the HTML, there are a couple of ways we could do this.\n",
    "\n",
    "* We could identify the cells by their tag `td`.\n",
    "\n",
    "* We could use the the class name `.detail`.\n",
    "\n",
    "* We could combine both and use the selector `td.detail`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell in rows[2].select('td'):\n",
    "    print(cell)\n",
    "print()\n",
    "\n",
    "for cell in rows[2].select('.detail'):\n",
    "    print(cell)\n",
    "print()\n",
    "\n",
    "for cell in rows[2].select('td.detail'):\n",
    "    print(cell)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that these are all the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert rows[2].select('td') == rows[2].select('.detail') == rows[2].select('td.detail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go with `td.detail` to be as specific as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# select only those 'td' tags with class 'detail'\n",
    "row = rows[2] \n",
    "detailCells = row.select('td.detail')\n",
    "detailCells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, we're interested in the actual **text** of a website, not its tags. Remember, to get the text of an HTML element, use the `text` member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the text in each of those cells\n",
    "rowData = [cell.text for cell in detailCells]\n",
    "\n",
    "print(rowData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Now we just use our basic Python knowledge to get the elements of this list that we want. Remember, we want the senator's name, their district, and their party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check em out\n",
    "print(rowData[0]) # Name\n",
    "print(rowData[3]) # district\n",
    "print(rowData[4]) # party"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Getting rid of junk rows\n",
    "\n",
    "We saw at the beginning that not all of the rows we got actually correspond to a senator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad rows\n",
    "print(rows[0])\n",
    "print()\n",
    "print(rows[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we write our for loop, we only want it to apply to the relevant rows. So we'll need to filter out the irrelevant rows. The way to do this is to compare some of these to the rows we do want, see how they differ, and then formulate that in a conditional.\n",
    "\n",
    "As you can imagine, there a lot of possible ways to do this, and it'll depend on the website. We'll show some here to give you an idea of how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad row\n",
    "print(len(rows[0]))\n",
    "print(len(rows[1]))\n",
    "\n",
    "# good row\n",
    "print(len(rows[2]))\n",
    "print(len(rows[3]))\n",
    "\n",
    "# maybe this will work?\n",
    "good_rows = [r for r in rows if len(r) == 5]\n",
    "\n",
    "# doesn't look like it\n",
    "print(good_rows[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad row\n",
    "print(rows[-1].select('td.detail'))\n",
    "print()\n",
    "\n",
    "# good row\n",
    "print(rows[5].select('td.detail'))\n",
    "print()\n",
    "\n",
    "# how about this?\n",
    "good_rows = [r for r in rows if r.select('td.detail')]\n",
    "\n",
    "print(\"Checking rows...\\n\")\n",
    "print(good_rows[0])\n",
    "print()\n",
    "print(good_rows[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we found something that worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Loop it all together\n",
    "\n",
    "Now that we've seen how to get the data we want from one row, as well as filter out the rows we don't want, let's put it all together into a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a GET request\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\n",
    "\n",
    "# read the content of the server’s response\n",
    "src = req.text\n",
    "\n",
    "# soup it\n",
    "soup = BeautifulSoup(src, \"lxml\")\n",
    "\n",
    "# Create empty list to store our data\n",
    "members = []\n",
    "\n",
    "# returns every ‘tr tr tr’ css selector in the page\n",
    "rows = soup.select('tr tr tr')\n",
    "\n",
    "# get rid of junk rows\n",
    "rows = [r for r in rows if r.select('td.detail')]\n",
    "\n",
    "# loop through all rows\n",
    "for row in rows:\n",
    "    # select only those 'td' tags with class 'detail'\n",
    "    detailCells = row.select('td.detail')\n",
    "        \n",
    "    # Keep only the text in each of those cells\n",
    "    rowData = [cell.text for cell in detailCells]\n",
    "    \n",
    "    # Collect information\n",
    "    name = rowData[0]\n",
    "    district = int(rowData[3])\n",
    "    party = rowData[4]\n",
    "    \n",
    "    # Store in a tuple\n",
    "    tup = (name,district,party)\n",
    "    \n",
    "    # Append to list\n",
    "    members.append(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be 61\n",
    "len(members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what we have in `members`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(members[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challege 3: Get HREF element pointing to members' bills. \n",
    "\n",
    "The code above retrieves information on:  \n",
    "\n",
    "    - the senator's name\n",
    "    - their district number\n",
    "    - and their party\n",
    "\n",
    "We now want to retrieve the URL for each senator's list of bills. The format for the list of bills for a given senator is:\n",
    "\n",
    "http://www.ilga.gov/senate/SenatorBills.asp + ? + GA=98 + &MemberID=**_memberID_** + &Primary=True\n",
    "\n",
    "to get something like:\n",
    "\n",
    "http://www.ilga.gov/senate/SenatorBills.asp?MemberID=1911&GA=98&Primary=True\n",
    "\n",
    "You should be able to see that, unfortunately, _memberID_ is not currently something pulled out in our scraping code.\n",
    "\n",
    "Your initial task is to modify the code above so that we also **retrieve the full URL which points to the corresponding page of primary-sponsored bills**, for each member, and return it along with their name, district, and party.\n",
    "\n",
    "Tips: \n",
    "\n",
    "* To do this, you will want to get the appropriate anchor element (`<a>`) in each legislator's row of the table. You can again use the `.select()` method on the `row` object in the loop to do this — similar to the command that finds all of the `td.detail` cells in the row. Remember that we only want the link to the legislator's bills, not the committees or the legislator's profile page.\n",
    "* The anchor elements' HTML will look like `<a href=\"/senate/Senator.asp/...\">Bills</a>`. The string in the `href` attribute contains the **relative** link we are after. You can access an attribute of a BeatifulSoup `Tag` object the same way you access a Python dictionary: `anchor['attributeName']`. (See the <a href=\"http://www.crummy.com/software/BeautifulSoup/bs4/doc/#tag\">documentation</a> for more details).\n",
    "* There are a _lot_ of different ways to use BeautifulSoup to get things done; whatever you need to do to pull that HREF out is fine.\n",
    "\n",
    "I've started out the code for you. Fill it in where it says `#YOUR CODE HERE` (Save the path into an object called `full_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a GET request\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\n",
    "\n",
    "# read the content of the server’s response\n",
    "src = req.text\n",
    "\n",
    "# soup it\n",
    "soup = BeautifulSoup(src, \"lxml\")\n",
    "\n",
    "# Create empty list to store our data\n",
    "members = []\n",
    "\n",
    "# returns every ‘tr tr tr’ css selector in the page\n",
    "rows = soup.select('tr tr tr')\n",
    "\n",
    "# get rid of junk rows\n",
    "rows = [r for r in rows if r.select('td.detail')]\n",
    "\n",
    "# loop through all rows\n",
    "for row in rows:\n",
    "    # select only those 'td' tags with class 'detail'\n",
    "    detailCells = row.select('td.detail')\n",
    "        \n",
    "    # Keep only the text in each of those cells\n",
    "    rowData = [cell.text for cell in detailCells]\n",
    "    \n",
    "    # Collect information\n",
    "    name = rowData[0]\n",
    "    district = int(rowData[3])\n",
    "    party = rowData[4]\n",
    "    \n",
    "    # YOUR CODE HERE.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Store in a tuple\n",
    "    tup = (name, district, party, full_path)\n",
    "    \n",
    "    # Append to list\n",
    "    members.append(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to test \n",
    "\n",
    "# members[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4: Make a function\n",
    "\n",
    "Turn the code above into a function that accepts a URL, scrapes the URL for its senators, and returns a list of tuples containing information about each senator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR FUNCTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to test your code!\n",
    "\n",
    "# senateMembers = get_members('http://www.ilga.gov/senate/default.asp?GA=98')\n",
    "# len(senateMembers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Scrape Bills\n",
    "****\n",
    "\n",
    "## 3.1 Writing a Scraper Function\n",
    "\n",
    "Now we want to scrape the webpages corresponding to bills sponsored by each bills.\n",
    "\n",
    "Write a function called `get_bills(url)` to parse a given Bills URL. This will involve:\n",
    "\n",
    "  - requesting the URL using the <a href=\"http://docs.python-requests.org/en/latest/\">`requests`</a> library\n",
    "  - using the features of the `BeautifulSoup` library to find all of the `<td>` elements with the class `billlist`\n",
    "  - return a _list_ of tuples, each with:\n",
    "      - description (2nd column)\n",
    "      - chamber (S or H) (3rd column)\n",
    "      - the last action (4th column)\n",
    "      - the last action date (5th column)\n",
    "      \n",
    "I've started the function for you. Fill in the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMPLETE THIS FUNCTION\n",
    "def get_bills(url):\n",
    "    src = requests.get(url).text\n",
    "    soup = BeautifulSoup(src)\n",
    "    rows = soup.select('tr')\n",
    "    bills = []\n",
    "    for row in rows:\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "               \n",
    "        tup = (bill_id, description, chamber, last_action, last_action_date)\n",
    "        bills.append(tup)\n",
    "    return(bills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment to test your code:\n",
    "# test_url = senateMembers[0][3]\n",
    "# get_bills(test_url)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Get all the bills\n",
    "\n",
    "Finally, create a dictionary `bills_dict` which maps a district number (the key) onto a list_of_bills (the value) eminating from that district. You can do this by looping over all of the senate members in `members_dict` and calling `get_bills()` for each of their associated bill URLs.\n",
    "\n",
    "NOTE: please call the function `time.sleep(1)` for each iteration of the loop, so that we don't destroy the state's web site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to test\n",
    "# bills_dict[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
